{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "bert-babble.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rISBLVKheTEy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "a6c35b69-9888-4a65-941f-760cad1b7d05"
      },
      "source": [
        "!pip3 install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 33.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 37.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 29.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 24.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 24.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 21.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 21.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 22.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 22.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 22.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 22.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.14.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FjAUrIOWeG4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "O2p3VIiVeG4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b94fd8ec-158d-464c-e67b-597a4389ba1e"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda(0)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
        "\n",
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
        "\n",
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:40<00:00, 10011741.10B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 363106.73B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoaZVIeWeG4l",
        "colab_type": "text"
      },
      "source": [
        "# Generations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTRWDFGDeG4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx]\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "    if top_k > 0:\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm7YVMvVeG4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "\n",
        "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]    \n",
        "    \n",
        "    return tokenize_batch(batch)\n",
        "\n",
        "def parallel_sequential_generation_conditioned(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True, sentiment=None):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    untokenized_batch = untokenize_batch(batch)\n",
        "    sentiment_id = tokenizer.convert_tokens_to_ids([sentiment])[0]\n",
        "    \n",
        "    ll = np.random.randint(0, max_len)\n",
        "    for jj in range(batch_size):            \n",
        "        batch[jj][seed_len+ll] = sentiment_id        \n",
        "    for ii in range(max_iter):\n",
        "        kk = np.random.randint(0, max_len)\n",
        "        while(kk == ll):\n",
        "            kk = np.random.randint(0, max_len)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "            \n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]        \n",
        "            \n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))\n",
        "            \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def generate_conditioned(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1, sentiment=None):\n",
        "    # main generation function to call\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    for batch_n in range(n_batches):\n",
        "        batch = parallel_sequential_generation_conditioned(seed_text, max_len=max_len, top_k=top_k,\n",
        "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                               cuda=cuda, verbose=False, sentiment=sentiment)\n",
        "                \n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3_8bQpJeG41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))\n",
        "    \n",
        "def read_sents(in_file, should_detokenize=False):\n",
        "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
        "    if should_detokenize:\n",
        "        sents = [detokenize(sent) for sent in sents]\n",
        "    return sents\n",
        "\n",
        "def write_sents(out_file, sents, should_detokenize=False):\n",
        "    with open(out_file, \"w\") as out_fh:\n",
        "        for sent in sents:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWZNrWqKeG46",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e9a7de77-467c-4385-c9dc-da66f2d2384a"
      },
      "source": [
        "n_samples = 100\n",
        "batch_size = 20\n",
        "max_len = 20\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "sentiment_lbl = 'happy'\n",
        "for temp in [1.0]:\n",
        "    bert_sents = generate_conditioned(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                          sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
        "                          cuda=True, sentiment=sentiment_lbl)\n",
        "    out_file = \"/content/generated_sentences/%s-len%d-burnin%d-topk%d-temp%.3f-sentiment%s.txt\" % (model_version, max_len, burnin, top_k, temp, sentiment_lbl)\n",
        "    write_sents(out_file, bert_sents, should_detokenize=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished batch 1 in 23.372s\n",
            "Finished batch 2 in 23.541s\n",
            "Finished batch 3 in 23.736s\n",
            "Finished batch 4 in 23.657s\n",
            "Finished batch 5 in 23.614s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93Fy-7f8eG5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_file = \"/content/generated_sentences/generations-len20-burnin200-temp0.700.txt\"\n",
        "bert_sents = read_sents(in_file, should_detokenize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "c5amYbiAeG5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(50):\n",
        "    printer(sents[i], should_detokenize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLz2jTLmeG5N",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtKmGzGUeG5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate import bleu_score as bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S1pMVqZeG5Z",
        "colab_type": "text"
      },
      "source": [
        "## Quality Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2a6xmj7eG5a",
        "colab_type": "text"
      },
      "source": [
        "How similar are the generated sentences to the original training data (Toronto Book Corpus and Wikipedia dumps). We follow Yu et al., (2017) and compute the BLEU between the generations and the test sets of both corpora by treating the test set as the references for each generation. The tests sets are large; we subsample 5000 examples from each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ-ylLMweG5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        " \n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "def prepare_tbc(data_file):        \n",
        "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
        "    return prepare_data(data_file, replacements=replacements)\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEDKJN3oeG5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki103_file = 'data/wiki103.5k.txt'\n",
        "tbc_file = 'data/tbc.5k.txt'\n",
        "\n",
        "wiki_data = prepare_wiki(wiki103_file)\n",
        "tbc_data = prepare_tbc(tbc_file)\n",
        "#sents = [detokenize(sent) for sent in sents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCbDHsrNeG5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"BERT-TBC BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data)))\n",
        "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, wiki_data)))\n",
        "print(\"BERT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data[:2500] + wiki_data[:2500])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxPvrhxAeG5t",
        "colab_type": "text"
      },
      "source": [
        "## Comparing to existing models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq3OHZkIeG5v",
        "colab_type": "text"
      },
      "source": [
        "The OpenAI Generative Pretraining Transformer is another pretrained model successfully used for transfer learning. Since the model is a unidirectional language model, we can straightforwardly generate from the model. See [this repo](https://github.com/huggingface/pytorch-openai-transformer-lm) by Thomas Wolf at Huggingface for instructions for setting up the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZBU9m28eG5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(1, os.path.join(\".\", \"pytorch-openai-transformer-lm\"))\n",
        "\n",
        "from model_pytorch import LMModel, load_openai_pretrained_model, DEFAULT_CONFIG\n",
        "from text_utils import TextEncoder\n",
        "\n",
        "def load_openai_gpt(n_special=1, n_ctx=512):\n",
        "    text_encoder = TextEncoder(\"pytorch-openai-transformer-lm/model/encoder_bpe_40000.json\", \n",
        "                               \"pytorch-openai-transformer-lm/model/vocab_40000.bpe\")\n",
        "    encoder = text_encoder.encoder\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    vocab = n_vocab + n_special + n_ctx\n",
        "\n",
        "    args = DEFAULT_CONFIG\n",
        "    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n",
        "    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special,\n",
        "                                 path=\"pytorch-openai-transformer-lm/model/\",\n",
        "                                 path_names=\"pytorch-openai-transformer-lm/\")\n",
        "    #lm_model.to(device)\n",
        "    lm_model.return_probs = False\n",
        "    lm_model.eval()\n",
        "    return lm_model, text_encoder\n",
        "\n",
        "def make_batch(X, n_vocab, n_special, batch_size):\n",
        "    X = np.array(X)\n",
        "    assert X.ndim in [1, 2]\n",
        "    if X.ndim == 1:\n",
        "        X = np.expand_dims(X, axis=0)\n",
        "    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n",
        "    pos_enc = np.tile(pos_enc, (batch_size, pos_enc.shape[-1])) #np.expand_dims(pos_enc, axis=0)\n",
        "    batch = np.stack([X, pos_enc], axis=-1)\n",
        "    batch = torch.tensor(batch, dtype=torch.long)#.to(device)\n",
        "    return batch\n",
        "\n",
        "def append_batch(X, next_idx):\n",
        "    next_pos = X[:, -1:, 1] + 1\n",
        "    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n",
        "    return torch.cat((X, next_x), 1)\n",
        "\n",
        "def _generate_sentence_openai(model, text_encoder, seed_text, batch_size=10, gen_len=20, \n",
        "                             topk=100, sample=True, n_special=0):\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    #X = np.random.randint(n_vocab, size=(batch_size, 1)).tolist()\n",
        "    #sents = [[text_encoder.decoder[X[i][0]]].replace('</w>', '') for i in range(batch_size)]\n",
        "    X = [[n_vocab - 1] for _ in range(batch_size)]\n",
        "    sents = [[] for _ in range(batch_size)]\n",
        "    if seed_text:\n",
        "        seed_ids = text_encoder.encode([seed_text,])\n",
        "        X = [X[i] + seed_ids[0] for i in range(batch_size)]\n",
        "        sents = [[seed_text] for _ in range(batch_size)]\n",
        "    XMB = make_batch(X, n_vocab, n_special, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    for step_n in range(gen_len):\n",
        "        out = model(XMB) + model.pos_emb_mask\n",
        "        next_idxs = generate_step(out, gen_idx=step_n, top_k=topk, sample=sample, return_list=False)\n",
        "        idxs = next_idxs.tolist()\n",
        "        for i in range(batch_size):\n",
        "            next_token = idxs[i]\n",
        "            if next_token == n_vocab:\n",
        "                next_token = \"<EOS>\"\n",
        "            else:\n",
        "                next_token = text_encoder.decoder[next_token].replace('</w>', '')\n",
        "            sents[i].append(next_token)\n",
        "        XMB = append_batch(XMB, next_idxs.unsqueeze(-1))\n",
        "        \n",
        "    return [[tok for tok in sent if tok != '\\n'] for sent in sents]\n",
        "\n",
        "def generate_openai(model, text_encoder, n_samples, seed_text, \n",
        "                    batch_size=10, gen_len=20, \n",
        "                    topk=100, temperature=temperature, sample=sample,\n",
        "                    n_special=0, print_every=1):\n",
        "    sents = []\n",
        "    start_time = time.time()\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    for batch_n in range(n_batches):\n",
        "        batch_sents = _generate_sentence_openai(model, text_encoder, seed_text,\n",
        "                                                batch_size=batch_size, gen_len=gen_len, \n",
        "                                                topk=topk, sample=sample,\n",
        "                                                n_special=n_special)\n",
        "        sents += batch_sents\n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Generated batch %d of %d in %.3fs\" % (batch_n + 1, n_batches, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "    return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Ej8-eq3oeG57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt_model, gpt_text_encoder = load_openai_gpt(n_special=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gWs829uJeG6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 1000\n",
        "batch_size = 50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "openai_sents = generate_openai(gpt_model, gpt_text_encoder, seed_text=\"\", \n",
        "                               n_samples=n_samples, batch_size=batch_size, gen_len=max_len,\n",
        "                               topk=top_k, temperature=temperature, sample=sample,\n",
        "                               n_special=1, print_every=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPmGXdNSeG6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "printer(openai_sents[9], should_detokenize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "m2Hu9OhNeG6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"GPT-TBC BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data)))\n",
        "print(\"GPT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, wiki_data)))\n",
        "print(\"GPT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data[:2500] + wiki_data[:2500])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn8DIrYmeG6Z",
        "colab_type": "text"
      },
      "source": [
        "## Diversity Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znu1buweeG6b",
        "colab_type": "text"
      },
      "source": [
        "Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference. Lower is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HockkITeG6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def self_bleu(sents):\n",
        "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
        "\n",
        "def get_ngram_counts(sents, max_n=4):\n",
        "    size2count = {}\n",
        "    for i in range(1, max_n + 1):\n",
        "        size2count[i] = Counter([n for sent in sents for n in ngrams(sent, i)])\n",
        "    return size2count\n",
        "\n",
        "def ref_unique_ngrams(preds, refs, max_n=4):\n",
        "    # get # of *distinct* pred ngrams that don't appear in ref\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    ref_ngrams = get_ngram_counts(refs, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        pred_ngram_counts = set(pred_ngrams[i].keys())\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        ref_ngram_counts = set(ref_ngrams[i].keys())\n",
        "        pct_unique[i] = len(pred_ngram_counts.difference(ref_ngram_counts)) / total\n",
        "    return pct_unique\n",
        "        \n",
        "def self_unique_ngrams(preds, max_n=4):\n",
        "    # get # of pred ngrams with count 1\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        pct_unique[i] = n_unique / total\n",
        "    return pct_unique"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ccDUAS9seG6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"BERT self-BLEU: %.2f\" % (100 * self_bleu(bert_sents)))\n",
        "print(\"OpenAI self-BLEU: %.2f\" % (100 * self_bleu(openai_sents)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yc_5kREeG6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = self_unique_ngrams(bert_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJHoFZB7eG6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pct_uniques = ref_unique_ngrams(openai_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = ref_unique_ngrams(openai_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = self_unique_ngrams(openai_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}