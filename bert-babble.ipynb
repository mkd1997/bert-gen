{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert-babble.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rISBLVKheTEy",
        "outputId": "c626583f-f0ca-4520-ac76-ff79b71f4869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip3 install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 24.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 6.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FjAUrIOWeG4M",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O2p3VIiVeG4c",
        "outputId": "443a114e-9a82-4e82-bfec-61835b7f8e22",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda(0)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
        "\n",
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
        "\n",
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:13<00:00, 30284833.70B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 929886.07B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LoaZVIeWeG4l"
      },
      "source": [
        "# Generations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FTRWDFGDeG4m",
        "colab": {}
      },
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx]\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "    if top_k > 0:\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rm7YVMvVeG4t",
        "colab": {}
      },
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "\n",
        "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]    \n",
        "    \n",
        "    return tokenize_batch(batch)\n",
        "\n",
        "def parallel_sequential_generation_conditioned(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True, sentiment=None):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    # untokenizing batch to add sentiment words\n",
        "    untokenized_batch = untokenize_batch(batch)\n",
        "    \n",
        "    # tokenizing sentiment words\n",
        "    sentiment_id = tokenizer.convert_tokens_to_ids([sentiment])[0]\n",
        "    \n",
        "    # inserting token at random position in each sentence of the batch\n",
        "    ll = np.random.randint(0, max_len)\n",
        "    for jj in range(batch_size):            \n",
        "        batch[jj][seed_len+ll] = sentiment_id\n",
        "        \n",
        "    for ii in range(max_iter):\n",
        "        \n",
        "        # inserting mask at random position such that it doesn't replace the sentiment word\n",
        "        kk = np.random.randint(0, max_len)\n",
        "        while(kk == ll):\n",
        "            kk = np.random.randint(0, max_len)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "            \n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]        \n",
        "            \n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))\n",
        "            \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def generate_conditioned(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1, sentiment=None):\n",
        "    # main generation function to call\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    for batch_n in range(n_batches):\n",
        "        batch = parallel_sequential_generation_conditioned(seed_text, max_len=max_len, top_k=top_k,\n",
        "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                               cuda=cuda, verbose=False, sentiment=sentiment)\n",
        "                \n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c3_8bQpJeG41",
        "colab": {}
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))\n",
        "    \n",
        "def read_sents(in_file, should_detokenize=False):\n",
        "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
        "    if should_detokenize:\n",
        "        sents = [detokenize(sent) for sent in sents]\n",
        "    return sents\n",
        "\n",
        "def write_sents(out_file, sents, should_detokenize=False):\n",
        "    with open(out_file, \"w\") as out_fh:\n",
        "        for sent in sents:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rWZNrWqKeG46",
        "outputId": "e1a412ef-12e5-46d6-b740-ca7a87e0f165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "n_samples = 1000\n",
        "batch_size = 20\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "sentiment_lbl = 'happy'\n",
        "for temp in [1.0]:\n",
        "    bert_sents = generate_conditioned(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                          sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
        "                          cuda=True, sentiment=sentiment_lbl)\n",
        "    out_file = \"/content/generated_sentences/%s-len%d-burnin%d-topk%d-temp%.3f-sentiment%s.txt\" % (model_version, max_len, burnin, top_k, temp, sentiment_lbl)\n",
        "    write_sents(out_file, bert_sents, should_detokenize=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished batch 1 in 43.344s\n",
            "Finished batch 2 in 44.020s\n",
            "Finished batch 3 in 43.898s\n",
            "Finished batch 4 in 44.019s\n",
            "Finished batch 5 in 43.932s\n",
            "Finished batch 6 in 43.915s\n",
            "Finished batch 7 in 43.933s\n",
            "Finished batch 8 in 44.020s\n",
            "Finished batch 9 in 43.971s\n",
            "Finished batch 10 in 43.979s\n",
            "Finished batch 11 in 43.935s\n",
            "Finished batch 12 in 43.952s\n",
            "Finished batch 13 in 44.026s\n",
            "Finished batch 14 in 43.945s\n",
            "Finished batch 15 in 44.030s\n",
            "Finished batch 16 in 44.054s\n",
            "Finished batch 17 in 44.033s\n",
            "Finished batch 18 in 43.977s\n",
            "Finished batch 19 in 43.950s\n",
            "Finished batch 20 in 43.988s\n",
            "Finished batch 21 in 44.001s\n",
            "Finished batch 22 in 44.014s\n",
            "Finished batch 23 in 43.946s\n",
            "Finished batch 24 in 43.969s\n",
            "Finished batch 25 in 43.987s\n",
            "Finished batch 26 in 43.976s\n",
            "Finished batch 27 in 43.979s\n",
            "Finished batch 28 in 43.997s\n",
            "Finished batch 29 in 44.034s\n",
            "Finished batch 30 in 44.040s\n",
            "Finished batch 31 in 43.990s\n",
            "Finished batch 32 in 44.025s\n",
            "Finished batch 33 in 43.967s\n",
            "Finished batch 34 in 44.038s\n",
            "Finished batch 35 in 44.004s\n",
            "Finished batch 36 in 43.906s\n",
            "Finished batch 37 in 44.006s\n",
            "Finished batch 38 in 44.070s\n",
            "Finished batch 39 in 44.053s\n",
            "Finished batch 40 in 44.004s\n",
            "Finished batch 41 in 43.975s\n",
            "Finished batch 42 in 44.007s\n",
            "Finished batch 43 in 44.100s\n",
            "Finished batch 44 in 44.074s\n",
            "Finished batch 45 in 44.008s\n",
            "Finished batch 46 in 44.065s\n",
            "Finished batch 47 in 44.016s\n",
            "Finished batch 48 in 44.001s\n",
            "Finished batch 49 in 44.027s\n",
            "Finished batch 50 in 44.036s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "93Fy-7f8eG5B",
        "colab": {}
      },
      "source": [
        "in_file = \"/content/generated_sentences/%s-len%d-burnin%d-topk%d-temp%.3f-sentiment%s.txt\" % (model_version, max_len, burnin, top_k, temp, sentiment_lbl)\n",
        "bert_sents = read_sents(in_file, should_detokenize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c5amYbiAeG5F",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "96140f0f-c409-452c-bc2a-032e3c369f3d"
      },
      "source": [
        "for i in range(50):\n",
        "    printer(bert_sents[i], should_detokenize=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here . jace ' s message was : think that you can stay here and think until they come . jace falling in love is too much . jace being happy for you is too much for me\n",
            "satan ' ( taken from virgil as showing his fear of satan ) means evil . ' run away ' ( \" stay away . run away \" ) means happy as it is ( or ) soon\n",
            ", also pregnant , hears the news of the newly family - run faith - based ministries , which she would like to continue : i ' ll never be happy . joan ( mary ) wilt\n",
            "gloria \" performed by mel clark . ( adapted from like a dove ) 2003 \" just like a dove edition \" of the original version of \" like a happy dove \" by bobby lavelli\n",
            "never again ! \" centuries and centuries and centuries and centuries - seeing a mastyr in another world tortured by a beautiful dragon twice his age to raise a happy child was one of her greatest nightmares\n",
            "to keep these poor people in such high spirits . for them ever coming back . yeah , there it was ... but you could never stop feeling sad and happy about it . if only it was\n",
            "retail price 5 million макс ( 100 million yen in - two year project period ) . the original version , today known as the romput happy , was released in russia and japan\n",
            "awful and piercing screams , the ones that littered the forest , tore through her , and she realized that this cry of pain came from a very loud , happy , and very cute , brown doe\n",
            "17 . the articles discuss on the subject ( tries to be \" funny \" ) . - 19 . articles on the topic : for example , i am happy , thanking her back to my mother\n",
            "liked living on the town , working . she had started taking other jobs in bon temps . they had worked so well alongside each other they were a bit happy being together here for the next week\n",
            "my god , here in this room is the world that they had sought . the world where their love was pleasured and they had enjoyed it . a happy world and they had lived in it\n",
            "february , philippe petit , who had already signed several players for the side and never rolled over , had said the season had been slow and was not very happy with the raw skill of the defence\n",
            "3 new versions of new song parts . includes new versions of ' mezna ' , this is my life , mind your music and maka you happy . plus deluxe version of new version\n",
            "good times , though , were over , remember ? granted , the days were blurry at night and the assembled grand masters never paid heed to the happy ' s dreams except when they woke\n",
            "not for you anna , not for you our lives would probably work out quicker and you will have a full role in our future . the world was so happy with how it worked out so much\n",
            "the past , it was a time when she would talk to casey , allie , and casey how things were going , and tell them how to be both happy and able to face their own uncertainty\n",
            "what is it that you see ? their eyes are green , but a different shade . they are tall and have a vibrant skin . they lead a very happy life , \" said jean - pierre\n",
            "helps calm josh down , telling him that she does not like working at the homeless shelter and the \" constant job pressures \" and josh ( whether he is happy or not being scared ) turns insane\n",
            "uttering a sound , i ruined nearly everything : my delicate diamond necklace returning to tin pan alley , the passionate embrace of my incredible lover ... and my happy ending - bye , bye , bye\n",
            "they must have never headed there , \" i mused . \" but they always ended up on the road , right after sunset . \" were they happy ( happy ) tourists , all give and take\n",
            "as a jingle writer and producer with bruce campbell and dewitt and young and happy . he is the musical director of spring fever and the heart of the universe , and a television and film composer\n",
            "a big project led by us high praise christian singer dave spink . \" happy birthday to you \" on \" est ... colone \" festival international paris , france , plateau du sud , france\n",
            "he finally looked three years old at emily , she blinked at first , not happy anymore . not like she ' d been a prisoner there for days . \" you on the run ? \" no\n",
            "little kids of his always seemed to want to protect her , to make her happy , not to fight . so she told herself to stay away , to keep him from thinking about her like that\n",
            "to wash away the scorn of his father and sister who he is not happy to see again ( later in the old man ' s chinese coffee shop ) , but timi is there somewhere\n",
            "was the old man in california . old and tired , old and tired and happy it was full of family , the elder of the moran family , his son and two daughters of the quinn family\n",
            "a laser beam ... the pinata ... the used sewing machine ... have a happy little family . i knew i had to , and the white house said yes in response twice , not three times\n",
            "lowe and arnott did offer positive reviews , although the film lacked any happy faces and motions for lowe and campbell to end their collaboration on rejoining he and his wife in their london home\n",
            ", no way . i looked up at the stars telling myself i was finally happy and free again . but no one there . say i love you . i started to stare at the front door\n",
            "will sleep for several minutes , but none of them rest . so , so happy . this should have piqued them , but still , sadly , now makes them want to see it along\n",
            "he also had the warm feeling that l longed for bloody marys , some happy woman , encouraged him to come . what he really wanted . . . was based on a secret plan . .\n",
            "piece of thought pulled at her , saying she was such a grown up , happy girl . more time to talk to mom . the bad days . the fact that she always kept her mouth shut\n",
            "32 and 33 : 33 let god give you our great cudgel , happy wife ! take with you the grain ! let his army of harvesters loot your lot , you amongst us\n",
            "of school kids , some new roommates , and a drug lab over in happy valley . two more wolves , much like me but with more and more rifles , lined up in a straight line\n",
            "pleasant little to him , always depending on its absolute practicality . like the happy beginning for his life of happiness . an oval - shaped blue vinyl display case lay out perfectly on the top shelf\n",
            "syringed and traumatized patients subsequently died and its \" bridesly happy \" waiting room was slated for even more serious treatment in march 2009 . center for cancer research grant ( 2006 )\n",
            ", artist ; remembering moses wilson ; the new artist ; two people come ; happy as you can ; john smith , artist , the san francisco art museum . south america . north america . europe\n",
            "fight my battles , to be alive and breathing . michael had chosen to be happy to be one , forever , with me . - \" put the sword down . \" - the room went silent\n",
            "on people , feed on life . people get their way outta it and be happy . i come into the kitchen and find my samoan mother . all she wears is a pair of jeans again\n",
            ", everyone in the school to celebrate ( seven times now ) , including her happy couple . the students cheering and laughing together . some children trying to get loose . some tumbling onto their rear seats\n",
            "cried for a few minutes ... we thought we were all so happy we could jump back off the ledge . it looks like saint anselm were down there , fighting a lot more foot soldiers\n",
            "cooperation with marvel comics ) go best of go home ! / happy ending of go home ! . november 2015 ; uk - itv released our end in my greatest nightmare 2015 ( via the bbc )\n",
            "our grace and his lordship comes to you , be free and happy in the praises of our grace and his lordship , and it is so for you . domenica sent her young friends to bed\n",
            "~ eva maria has sung some of the songs for ... always happy , los intangibles , somos la perdido , atras de ses menos and a baladas\n",
            "journey had been difficult and always short . he had longed for happy children - - for adults , he ' d often called them - - for two hundred years his parents had agreed to marry him\n",
            "fine arts foundation special collection . may 1991 . the happy child movement , october 2004 . \" political science education as what they are : political science texts and criticism \" , stephen kenyon\n",
            "doctor is even talking to me ! if only i had these happy memories of tomi ! these brilliant pink cheeks : sticky - mush , these otherworldly yet magical memories of tomi\n",
            "2 , 4 june 1983 ] shakespeare meet henry shakespeare - a happy life of henry shakespeare , william shakespeare . london : forstall ( [ 1983 ] ) . [ 2 , 4 june ]\n",
            "he turned away . \" do whatever you can to make them happy move through . \" \" oh please stop it , dad , \" milla giggled . \" please , \" dad shushed her\n",
            "knew this , but it was easy to see just how genuinely happy he had been that he could be with emma . he would take the absolute utmost care about what was to happen here to him\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x79i0eTL4oTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}